# GENERATE TIME: Tue Jun  3 11:25:47 2025
# CMD:
# train_grpo.py -opt /users/xiaokangliu/projects/FAR-GRPO/options/train/far/long_video_prediction/FAR_M_GRPO_Long_minecraft_res128_1000K_bs32.yml

name: v11_FAR_M_Long_minecraft_res128_1000K_bs32_GRPO
manual_seed: 0
mixed_precision: bf16

# dataset and data loader settings
datasets:
  train:
    type: MinecraftDataset
    data_list: /users/xiaokangliu/datasets/minecraft/minecraft_train.json
    split: training
    use_latent: false
    data_cfg:
      num_frames: 300
      frame_interval: 1
    batch_size_per_gpu: 8
    num_rollouts_per_prompt: 8 # currently for debug

  sample:
    type: MinecraftDataset
    data_list: /users/xiaokangliu/datasets/minecraft/minecraft_val.json
    split: validation
    data_cfg:
      num_frames: 300
      frame_interval: 1
    num_sample: 256
    batch_size_per_gpu: 8

models:
  model_cfg:
    transformer:
      from_pretrained: ~ # diffusers pretrained path (from_pretrained)
      init_cfg:
        type: FAR_M_Long
        config: {
          "condition_cfg": {
            "type": "action",
            "num_action_classes": 4
          }
        }
        pretrained_path: /users/xiaokangliu/models/FAR_M_Long_Minecraft_Action128-4c041561.pth
    lora_config:
        r: 8  # LoRA rank
        alpha: 8  # LoRA scaling factor
        dropout: 0.1
        target_modules: ["to_q", "to_k", "to_v", "to_out.0"]  # Target attention modules for LoRA
        bias: "none"
        task_type: "CAUSAL_LM"
    vae:
      type: MyAutoencoderDC
      from_config: options/model_cfg/dcae/model_16x_c32_config.json
      from_config_pretrained: /users/xiaokangliu/models/DCAE_Minecraft_Res128-a5677f66.pth
    scheduler:
      from_pretrained: options/model_cfg/far/scheduler_config.json
  clean_context_ratio: 0.1
  weighting_scheme: logit_normal
  training_type: long_context

# path
path:
  pretrain_network: ~

# training settings
train:
  train_pipeline: FARTrainer

  optim_g:
    type: AdamW
    lr: !!float 1e-5
    weight_decay: 0
    betas: [ 0.9, 0.999 ]

  param_names_to_optimize: ~
  ema_decay: 0.9999

  lr_scheduler: constant_with_warmup
  warmup_iter: 5000
  total_iter: 1000000
  max_grad_norm: 1.0
  adv_clip_max: 5
  # clip_range: 1e-4
  clip_range: 1e-3
  num_batches_per_epoch: 2

# validation settings
val:
  val_pipeline: FARPipeline
  val_freq: 5000
  eval_on_start: false
  sample_cfg:
    context_length: 144
    # unroll_length: 56
    unroll_length: 12
    guidance_scale: 1.5
    # num_inference_steps: 50
    num_inference_steps: 30
    sample_size: 8
    sample_steps_prob: 1
    sample_trajectory_per_video: 1
    use_kv_cache: true
  eval_cfg:
    metrics: ['mse', 'psnr', 'ssim', 'fvd', 'lpips']

# logging settings
logger:
  print_freq: 50
  save_checkpoint_freq: !!float 5000
  use_wandb: true
